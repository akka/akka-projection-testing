akka.http.server.request-timeout = 600 s

akka {
  loglevel = DEBUG

  actor {
    provider = cluster
    serialization-bindings {
      "akka.projection.testing.CborSerializable" = jackson-cbor
    }
  }

  remote {
    artery {
      canonical.port = 2552
    }
  }


  cluster {
    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"

    roles = ["write-model", "read-model"]

    sharding.least-shard-allocation-strategy.rebalance-absolute-limit = 20
    sharding.passivate-idle-entity-after = 3600s # clean up quickly between tests
  }

  persistence {

    journal-plugin-fallback {
      circuit-breaker {
        max-failures = 10
        call-timeout = 30s
        reset-timeout = 30s
      }
    }

    max-concurrent-recoveries = 50
  }

  projection {
      # The strategy to use to recover from unhandled exceptions without causing the projection to fail
  #     recovery-strategy {
  #       strategy = retry-and-fail
  #       retries = 3
  #       retry-delay = 250ms
  #     }

    # The configuration to use to restart the projection after an underlying streams failure
    # The Akka streams restart source is used to facilitate this behaviour
    # See the streams documentation for more details
    restart-backoff {
      min-backoff = 200ms
      max-backoff = 2s
      random-factor = 0.2

      # -1 will not cap the amount of restarts
      # 0 will disable restarts
      max-restarts = -1
    }

    grouped {
      group-after-envelopes = 20
      group-after-duration = 1000 ms
    }
  }

  management {
    cluster.bootstrap {
      contact-point-discovery {
        discovery-method = kubernetes-api
        required-contact-point-nr = 1
        required-contact-point-nr = ${?REQUIRED_CONTACT_POINT_NR}
      }
    }
  }
}

event-processor {
  # tags per projection
  # Increase for realisitc testing
  parallelism = 8
  # how many projections to run. Each will do the same thing
  # and the end result is validated for all
  # each projectin uses its own set of tags to increase load on the tag_* tables as it is better if those
  # reads/writes fail for creating issues than writes to the messages table
  # Increasing this isn't that realistic but does put a large load on the events by tag infrastructure
  nr-projections = 1

  # Set to on for performance testing without validation
  read-only = off

  # fail one in every N messages, causing a restart of the projection
  projection-failure-every = "off"
}

test {
  # Overridden based on the remoting port
  http.port = 8080

  #throttle-actors-per-second = 500
  throttle-actors-per-second = off
}


# cinnamon
cinnamon.prometheus {
  exporters += http-server
}

cinnamon.akka {
  persistence.entities {
    // sharded:? will expand to /system/sharding/?/*
    "sharded:?" {
      report-by = group
    }
  }

  cluster {
    shard-region-info = on
    node-metrics = on
  }
}

//cinnamon.akka.actors {
//  "configurable" {
//    report-by = group
//    includes = ["/system/sharding/configurable/*"]
//    excludes = ["akka.cluster.sharding.Shard"]
//  }
//}
