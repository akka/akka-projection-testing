alocalhostkka.http.server.request-timeout = 600 s
akka.cluster.sharding.passivate-idle-entity-after = 20s # clean up quickly between tests

akka {
  projection {
    jdbc {
      dialect = "postgres-dialect"
      offset-store {
        schema = ""
        table = "AKKA_PROJECTION_OFFSET_STORE"
      }
      blocking-jdbc-dispatcher {
        type = Dispatcher
        executor = "thread-pool-executor"
        thread-pool-executor {
          fixed-pool-size = 10
        }
      }
    }


    # The strategy to use to recover from unhandled exceptions without causing the projection to fail
    recovery-strategy {
      # fail - If the first attempt to invoke the handler fails it will immediately give up and fail the stream.
      # skip - If the first attempt to invoke the handler fails it will immediately give up, discard the element and
      #        continue with next.
      # retry-and-fail - If the first attempt to invoke the handler fails it will retry invoking the handler with the
      #                  same envelope this number of `retries` with the `delay` between each attempt. It will give up
      #                  and fail the stream if all attempts fail.
      # retry-and-skip - If the first attempt to invoke the handler fails it will retry invoking the handler with the
      #                  same envelope this number of `retries` with the `delay` between each attempt. It will give up,
      #                  discard the element and continue with next if all attempts fail.

      # seeing some duplicate events, skipping for now to focus on missed events issue
      strategy = retry-and-skip

      # The number of times to retry handler function
      # This is only applicable to `retry-and-fail` and `retry-and-skip` recovery strategies
      retries = 2

      # The delay between retry attempts
      # Only applicable to `retry-and-fail` and `retry-and-skip` recovery strategies
      retry-delay = 250ms
    }

    # The configuration to use to restart the projection after an underlying streams failure
    # The Akka streams restart source is used to facilitate this behaviour
    # See the streams documentation for more details
    restart-backoff {
      min-backoff = 200ms
      max-backoff = 2s
      random-factor = 0.2

      # -1 will not cap the amount of restarts
      # 0 will disable restarts
      max-restarts = -1
    }


  }


  loglevel = DEBUG

  actor {
    provider = cluster

    serialization-bindings {
      "akka.projection.testing.CborSerializable" = jackson-cbor
    }
  }


  cluster {
    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
  }

  # use Cassandra to store both snapshots and the events of the persistent actors
  persistence {

//    journal.plugin = "akka.persistence.cassandra.journal"
//    snapshot-store.plugin = "akka.persistence.cassandra.snapshot"

    # uncomment to use postgres as the event store
    journal.plugin = "jdbc-journal"
    snapshot-store.plugin = "jdbc-snapshot-store"

    journal-plugin-fallback {
      circuit-breaker {
        max-failures = 10
        call-timeout = 30s
        reset-timeout = 30s
      }
    }

    max-concurrent-recoveries = 50
  }

}

# Configuration for akka-persistence-cassandra
akka.persistence.cassandra {
  journal {
    keyspace = "akka_testing"
  }

  events-by-tag {
    bucket-size = "Hour"
    # for reduced latency
    eventual-consistency-delay = 500ms
    flush-interval = 50ms
    pubsub-notification = on
    first-time-bucket = "20201001T00:00"
  }

  query {
    refresh-interval = 2s
  }

  # don't use autocreate in production
  journal.keyspace-autocreate = on
  journal.tables-autocreate = on
  snapshot.keyspace-autocreate = on
  snapshot.tables-autocreate = on

  cleanup {
    dry-run = false
  }
}

datastax-java-driver {
  advanced.reconnect-on-init = on
}

# This will be overridden by the oeprator akka-persistence-jdbc plugin
jdbc-connection-settings {
  driver = "org.postgresql.Driver"

  # the following properties must be filled with the production values
  # they can be set using -D arguments, eg: -jdbc-connection-settings.user=the-production-user
  url = "jdbc:postgresql://localhost:5432/docker?reWriteBatchedInserts=true"
  user = "docker"
  password = "docker"

  # the following properties are used to configure the
  # Hikari connection pool used on the read-side (akka-projections)
  connection-pool {
    # How many connections should be available to from the pool?
    # it's recommended to use the same value used by the blocking-jdbc-dispatcher (see above)
    max-pool-size = ${akka.projection.jdbc.blocking-jdbc-dispatcher.thread-pool-executor.fixed-pool-size}

    # How long should we wait (in millis) before it times out?
    # In a normal scenario, we should always be able to get a connection
    # If we got a thread from the blocking-jdbc-dispatcher, we should be able to get a connection.
    # If for some reason the pool can't provide a connection, it's better to let it crash and liberate the current thread.
    # Hence the low timout (note, 250 is lowest value hikari accepts)
    timeout = 250ms
  }
}

akka-persistence-jdbc {
  shared-databases {
    default {
      profile = "slick.jdbc.PostgresProfile$"
      db {
        host = "localhost"
        url = ${jdbc-connection-settings.url}
        user = ${jdbc-connection-settings.user}
        password = ${jdbc-connection-settings.password}
        driver = ${jdbc-connection-settings.driver}
        numThreads = 5
        maxConnections = 5
        minConnections = 1
      }
    }
  }
}

jdbc-journal {
  use-shared-db = "default"
}

# the akka-persistence-snapshot-store in use
jdbc-snapshot-store {
  use-shared-db = "default"
}

# the akka-persistence-query provider in use
jdbc-read-journal {
  use-shared-db = "default"
}

event-processor {
  # tags per projection
  # Increase for realisitc testing
  parallelism = 4
  # how many projections to run. Each will do the same thing
  # and the end result is validated for all
  # each projectin uses its own set of tags to increase load on the tag_* tables as it is better if those
  # reads/writes fail for creating issues than writes to the messages table
  # Increasing this isn't that realistic but does put a large load on the events by tag infrastructure
  nr-projections = 1
}

test {
  # Overridden based on the remoting port
  http.port = 8080
  # fail one in every N messages, causing a restart of the projection
  projection-failure-every = "off"
}


# cinnamon
cinnamon.prometheus {
  exporters += http-server
}

cinnamon.akka {
  persistence.entities {
    // sharded:? will expand to /system/sharding/?/*
    "sharded:?" {
      report-by = group
    }
  }

  cluster {
    shard-region-info = on
    node-metrics = on
  }
}

//cinnamon.akka.actors {
//  "configurable" {
//    report-by = group
//    includes = ["/system/sharding/configurable/*"]
//    excludes = ["akka.cluster.sharding.Shard"]
//  }
//}
